{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ev1025/DA_Study/blob/main/%ED%81%AC%EB%A1%A4%EB%A7%81_%EC%97%B0%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFbLRRHMpp6m"
      },
      "outputs": [],
      "source": [
        "pip install webdriver_manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpTZZjy7pp6r"
      },
      "outputs": [],
      "source": [
        "pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBFJKuzVpp6s"
      },
      "outputs": [],
      "source": [
        "pip install BeautifulSoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KCA5WyDpp6s"
      },
      "source": [
        "# 카카오맵 크롤링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uZnhhUwpp6w"
      },
      "outputs": [],
      "source": [
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# 크롬 옵션\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('headless')\n",
        "options.add_argument('--disable-gpu--')\n",
        "options.add_argument('lang=ko_KR')\n",
        "\n",
        "# 크롬 드라이버 설치\n",
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "\n",
        "# 카카오맵 방문\n",
        "driver.get(\"https://map.kakao.com/\")\n",
        "\n",
        "# 페이지 로드시간 암묵적 대기\n",
        "driver.implicitly_wait(10)\n",
        "\n",
        "# csv 입력받을 DataFrame\n",
        "dog_map_df = pd.DataFrame(columns=('name','star','adr','category'))\n",
        "\n",
        "# 검색 할 목록\n",
        "search_list = ['서울 애견 미용실','서울 애견호텔', '서울 반려동물 미용실', '서울 동물약국',\\\n",
        "                 '서울 동물병원', '서울 강아지 수제간식','서울 애견 수제간식', '서울 애견동반', '서울 강아지유치원','서울 애견유치원', '서울 애견용품']\n",
        "\n",
        "# 목록에서 하나씩 검색\n",
        "for search_id in search_list:\n",
        "    # 검색상자 찾기\n",
        "    search_box = driver.find_element(By.CSS_SELECTOR, \"#search\\.keyword\\.query\")\n",
        "    # 검색할 단어 입력\n",
        "    search_box.send_keys(f\"{search_id}\")\n",
        "    time.sleep(1)\n",
        "    # 검색버튼 누르기\n",
        "    search_box.send_keys(Keys.ENTER)\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 지도설정 팝업 제거(최초 1회만 나타나서 예외구문 적용)\n",
        "    try:\n",
        "        driver.find_element(By.CSS_SELECTOR, \"body > div.coach_layer.coach_layer_type1 > div > div > div > span\").click()\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 최초에 페이지번호 뜨지 않고 더보기 눌러줘야함(최초 1회만 나타나서 예외구문 적용)\n",
        "    try:\n",
        "        more_btn = driver.find_element(By.CSS_SELECTOR, \"#info\\.search\\.place\\.more\")\n",
        "        more_btn.click()\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 검색결과의 총 개수 / 75(5페이지 당 요소 개수)\n",
        "    all_page = \"document.querySelector('.keywordSearch').innerHTML\"\n",
        "    page_raw = driver.execute_script(\"return \" + all_page)\n",
        "    page_html = BeautifulSoup(page_raw, 'html.parser')\n",
        "    all_page_html = page_html.find('em', class_=\"cnt\", id=\"info.search.place.cnt\").text\n",
        "    all_page_html = re.sub(r'[^0-9]', '', all_page_html)\n",
        "    all_page_num = math.ceil(int(all_page_html.text) / 75)\n",
        "\n",
        "    # dog_map_df에 입력받을 컬럼\n",
        "    name = []\n",
        "    star = []\n",
        "    adr = []\n",
        "    category = []         \n",
        "    \n",
        "    # 크롤링 시작\n",
        "    # 페이지당 15개의 개체가 있다.\n",
        "    # 이전, 페이지 1~5, 다음 버튼이 존재하며, 이전,다음버튼은 5개씩 넘어가게 되어있음\n",
        "    \n",
        "    # 다음버튼 누르는 횟수 개체 개수 = (15개 x 5page x 다음버튼 횟수)\n",
        "    for p in range(all_page_num):\n",
        "        # 1page ~ 5page 페이지이동 (6~10, 11~15 ...)\n",
        "        # page버튼이 모자르면 종료(6~10구간에서 6,7,8까지만 있으면 8page에서 종료)\n",
        "        for j in range(1,6):\n",
        "            try:\n",
        "                next_btn_1stp = driver.find_element(By.CSS_SELECTOR, f\"#info\\.search\\.page\\.no{j}\")\n",
        "                next_btn_1stp.click()\n",
        "            except:\n",
        "                print(f'{search_id}수집 끝')\n",
        "                break\n",
        "            time.sleep(1)\n",
        "\n",
        "            # 추출할 범위를 CSS로 선택해서 HTML형식으로 변환\n",
        "            js_script = \"document.querySelector('.placelist').innerHTML\"\n",
        "            # 자바스크립트형식 읽어들이기\n",
        "            raw = driver.execute_script(\"return \" + js_script)\n",
        "            html = BeautifulSoup(raw, 'html.parser')\n",
        "            time.sleep(2)\n",
        "\n",
        "            # 직접적인 데이터가 있는 CSS그룹 선택\n",
        "            contents = html.find_all(class_='PlaceItem clickArea')\n",
        "            time.sleep(2)\n",
        "\n",
        "            # CSS그룹에서 필요한 값 추출\n",
        "            for i in contents:\n",
        "                cr_name = i.find(class_='link_name').text\n",
        "                name.append(cr_name)\n",
        "\n",
        "                try:\n",
        "                    cr_star = float(i.select_one(\"em.num\").text)\n",
        "                    star.append(cr_star)\n",
        "                except:\n",
        "                    star.append(0)\n",
        "\n",
        "                cr_adr = i.select_one('.addr').text.replace('\\n', '').split(' ')\n",
        "                if cr_adr[0] == '서울':\n",
        "                    adr.append(cr_adr[1])\n",
        "\n",
        "                cr_category = i.find(class_='subcategory clickable').text\n",
        "                category.append(cr_category)\n",
        "                \n",
        "\n",
        "        next_btn = driver.find_element(By.XPATH,'//*[@id=\"info.search.page.next\"]')\n",
        "        next_btn.send_keys(Keys.ENTER)\n",
        "\n",
        "   \n",
        "    # 추출한 데이터 입력\n",
        "    search_data = pd.DataFrame({'name':name,'star':star,'adr':adr,'category':category})\n",
        "    dog_map_df = pd.concat([dog_map_df,search_data]) \n",
        "  \n",
        "    # 검색어 지우기\n",
        "    driver.implicitly_wait(3)\n",
        "    search_box.clear()\n",
        "    \n",
        "\n",
        "# 중복제거\n",
        "dog_map_df.drop_duplicates(['name'], keep='first', inplace=True)\n",
        "\n",
        "# csv파일로 저장\n",
        "dog_map_df.to_csv(path_or_buf=f'./dog_map_data.csv',encoding='utf-8-sig', index=False)\n",
        "\n",
        "# 크롬 웹페이지를 닫음\n",
        "driver.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CTH7Iz2pp6z"
      },
      "source": [
        "### 네이버 API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01MQ6UvQpp6z",
        "outputId": "1ef365d6-9ef3-4f7e-e5e7-db545f1d7636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "서울 동물병원 완료\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from urllib.parse import quote     # 한글 -> ASCII코드\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# 네이버 API 아이디, 비번\n",
        "client_id = \"aX81_3OCuF5Iyf5bgJmv\"\n",
        "client_secret = \"2QXdjTqRkD\"\n",
        "\n",
        "headers = {\n",
        "   'X-Naver-Client-Id':  client_id,\n",
        "   'X-Naver-Client-Secret': client_secret\n",
        "}\n",
        "# API주소\n",
        "url = \"https://openapi.naver.com/v1/search/local.json\"\n",
        "\n",
        "# 검색어 목록\n",
        "search =['서울 동물병원']\n",
        "\n",
        "# 검색어 마다 실행\n",
        "for query in search:\n",
        "    disp = '5'       # 출력개수(없어도됨)\n",
        "    start = '1'      # 시작번호(없어도됨)\n",
        "\n",
        "    params = {'query' : query,\n",
        "            'display' : disp,\n",
        "            'start' : start}\n",
        "\n",
        "    resp = requests.get(url, headers=headers, params=params)\n",
        "\n",
        "    # if resp.status_code ==200:\n",
        "    #   print(resp.json().get('items'))\n",
        "\n",
        "\n",
        "    # 요청결과 저장\n",
        "    search_list = []\n",
        "\n",
        "    for i in range(5):\n",
        "        temp = []\n",
        "        temp.append(resp.json().get('items')[i]['title'])\n",
        "        temp.append(resp.json().get('items')[i]['category'])\n",
        "        temp.append(resp.json().get('items')[i]['address'].split()[0])\n",
        "        temp.append(resp.json().get('items')[i]['address'].split()[1])\n",
        "        temp.append(resp.json().get('items')[i]['mapx'])\n",
        "        temp.append(resp.json().get('items')[i]['mapy'])\n",
        "        temp.append(resp.json().get('items')[i]['link'])\n",
        "        search_list.append(temp)\n",
        "\n",
        "    # csv 저장\n",
        "    # 파일명 /'w'쓰기모드 / 한글인코딩 / 자동줄바꿈 하지않음\n",
        "    f = open(f'{query}.csv', 'w', encoding='utf-8-sig', newline='') \n",
        "    csvwriter = csv.writer(f)\n",
        "\n",
        "     # 컬럼명 입력\n",
        "    csvwriter.writerow(['이름','업종','시','행정구','mapx','mapy','링크주소'])\n",
        "\n",
        "    # 행입력(위에 search_list 값 입력)\n",
        "    for j in search_list: \n",
        "        csvwriter.writerow(j)\n",
        "        \n",
        "    f.close()\n",
        "\n",
        "    # 하나의 검색어 저장이 완료되면 출력\n",
        "    print(f'{query} 완료')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqaVopo2pp61"
      },
      "source": [
        "### 탑텐몰 제품, 가격, 할인률"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciyoLSxcpp62"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "driver = webdriver.Chrome(\"C:/Users/eg287/chromedriver\") # 나의 웹드라이버의 위치(설정-도움말-버전에서 확인 후 최신 드라이버 사용)\n",
        "driver.get(\"https://www.topten10mall.com/kr/front/search/categorySearch.do?ctgNo=37341\") # 웹사이트 방문\n",
        "\n",
        "# 팝업 창 제거\n",
        "# driver.find_element(By.CSS_SELECTOR, \"button#intro_popup_close\").click()\n",
        "driver.implicitly_wait(10) # 페이지 로드시간에 10초 암묵적 대기\n",
        "\n",
        "# 검색창에 검색어 입력하기\n",
        "search_box = driver.find_element(By.CSS_SELECTOR, \"#searchWord\")\n",
        "search_box.send_keys(\"맨투맨\")\n",
        "\n",
        "time.sleep(3)\n",
        "\n",
        "# 검색버튼 누르기\n",
        "search_box.send_keys(Keys.ENTER)\n",
        "# raw = driver.page_source # 현재 랜더링된 페이지의 정보를 가져옴\n",
        "\n",
        "# 추출한 데이터 저장할 DataFrame 생성\n",
        "empty = pd.DataFrame(columns=['name','price','discount'])\n",
        "num = 0 # 추출한 값을 넣을 인덱스번호\n",
        "\n",
        "# 크롤링\n",
        "for p in range(28):\n",
        "    # 5초 delay\n",
        "    time.sleep(2)\n",
        "    \n",
        "    # 자바로 HTML가져오기(추출할 범위)\n",
        "    js_script = \"document.querySelector('#divList').innerHTML\"\n",
        "    raw = driver.execute_script(\"return \" + js_script)\n",
        "    \n",
        "    # requests로 HTML 가져오기\n",
        "    # raw = requests.get(\"https://www.topten10mall.com/kr/front/search/totalSearch.do?searchTerm=%EC%85%94%EC%B8%A0\")\n",
        "\n",
        "    # 파서로 추출할 대상 선택\n",
        "    html = BeautifulSoup(raw, 'html.parser')\n",
        "    contents = html.select('.card-goods__body')\n",
        "\n",
        "    # 추출대상별 구체적으로 추출할 클래스 값 추출\n",
        "    for i in contents:\n",
        "        name = i.select_one('.card-goods__text').text\n",
        "        try:\n",
        "            price = i.select_one('.card-goods__price').text\n",
        "        except:\n",
        "            price = 'NULL'\n",
        "        try:\n",
        "            discount = i.select_one('.card-goods__discount').text\n",
        "        except:\n",
        "            discount = '0%'\n",
        "\n",
        "        # empty 데이터프레임에 추출한 값 넣기\n",
        "        empty_dict = {'name':name, 'price':price, 'discount':discount}\n",
        "        for key,value in empty_dict.items():\n",
        "            empty.loc[num,key] = value\n",
        "        num += 1 # 인덱스 번호 1씩 증가하도록\n",
        "        # print(name, price, discount)\n",
        "        \n",
        "        \n",
        "    # 다음 페이지로 이동(다음페이지 없을때까지)\n",
        "    try:\n",
        "        next_btn = driver.find_element(By.CSS_SELECTOR, '#searchGoods > nav > ul > li:nth-last-child(1)')\n",
        "        next_btn.click()\n",
        "    except:\n",
        "        print(\"데이터 수집 완료\")\n",
        "        break\n",
        "\n",
        "\n",
        "# empty.to_csv(path_or_buf='./크롤링.csv',index=False, encoding = \"utf-8-sig\")\n",
        "len(empty)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbmnrMfNpp63"
      },
      "source": [
        "### 네이버 크롤링 실패"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs6jsPM_pp63"
      },
      "outputs": [],
      "source": [
        "# 네이버 안돼\n",
        "url = 'https://map.naver.com/v5/favorite/myPlace/folder/74b5e2268e0444c999227a4ec63e2839?c=10.53,0,0,0,dh'\n",
        "html = requests.get(url)\n",
        "soup = BeautifulSoup(html.content,'html.parser')\n",
        "soup\n",
        "# data = soup.select('.\\_2gfp\\-T')\n",
        "# data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CFiHu0_pp64"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "driver = webdriver.Chrome(\"C:/Users/eg287/chromedriver\") # 나의 웹드라이버의 위치(설정-도움말-버전에서 확인 후 최신 드라이버 사용)\n",
        "driver.get(\"https://map.naver.com/v5/?c=15,0,0,0,dh\")            # 웹사이트 방문\n",
        "\n",
        "\n",
        "# 팝업 창 제거\n",
        "# driver.find_element(By.CSS_SELECTOR, \"button#intro_popup_close\").click()\n",
        "driver.implicitly_wait(10) # 페이지 로드시간에 10초 암묵적 대기\n",
        "\n",
        "search_list = ['서울 동물병원']\n",
        "# ,'서울 반려동물 미용실', '서울 강아지 간식', '서울 애견동반', '서울 강아지호텔', '서울 강아지유치원'\n",
        "for search_id in search_list:\n",
        "\n",
        "    # 검색창에 검색어 입력하기\n",
        "    search_box = driver.find_element(By.CSS_SELECTOR, \"div.input_box>input.input_search\")\n",
        "    search_box.send_keys(f\"{search_id}\")\n",
        "\n",
        "    time.sleep(3)\n",
        "\n",
        "    # 검색버튼 누르기\n",
        "    search_box.send_keys(Keys.ENTER)\n",
        "\n",
        "    name = []\n",
        "    adr = []\n",
        "    category = []\n",
        "\n",
        "\n",
        "    # 크롤링\n",
        "    for p in range(1):\n",
        "            #5초 delay\n",
        "            time.sleep(2)\n",
        "            \n",
        "            js_script = \"document.querySelector('*').innerHTML\"\n",
        "            raw = driver.execute_script(\"return \" + js_script)\n",
        "            html = BeautifulSoup(raw, 'html.parser')\n",
        "            \n",
        "            contents = html.find_all(class_='VLTHu OW9LQ')\n",
        "\n",
        "            for i in contents:\n",
        "                name.append(i.select_one(\" span.place_bluelink YwYLL\").text)\n",
        "\n",
        "                address = i.select_one('#_pcmap_list_scroll_container > ul > li:nth-child(1) > div.qbGlu > div.ouxiq.icT4K > div > div > span > a > span.hClKF').text.replace('\\n', '').split(' ')\n",
        "                if address[0] =='서울':\n",
        "                    adr.append(address[1])\n",
        "                \n",
        "                category.append(i.select_one('#_pcmap_list_scroll_container > ul > li:nth-child(1) > div.qbGlu > div.ouxiq.icT4K > a:nth-child(1) > div > div > span.YzBgS').text)\n",
        "            \n",
        "            try:    \n",
        "                next_btn = driver.find_element(By.CSS_SELECTOR, \"#app-root > div > div.XUrfU > div.zRM9F > a:nth-child(7) > svg\")\n",
        "                next_btn.click()\n",
        "            except:\n",
        "                print(\"데이터 수집 완료\")\n",
        "                break\n",
        "        \n",
        "    \n",
        "    search_data = pd.DataFrame({'name':name,'adr':adr,'category':category})        \n",
        "    search_data.to_csv(path_or_buf=f'./{search_id}.csv',encoding='utf-8-sig', index=False)\n",
        "    \n",
        "    # 검색어 지우기\n",
        "    search_box.clear()\n",
        "\n",
        "\n",
        "\n",
        "# 크롬 웹페이지를 닫음\n",
        "# driver.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
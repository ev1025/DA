{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ev1025/DA_Study/blob/main/%ED%81%AC%EB%A1%A4%EB%A7%81_%EC%97%B0%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFbLRRHMpp6m"
      },
      "outputs": [],
      "source": [
        "pip install webdriver_manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpTZZjy7pp6r"
      },
      "outputs": [],
      "source": [
        "pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBFJKuzVpp6s"
      },
      "outputs": [],
      "source": [
        "pip install BeautifulSoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KCA5WyDpp6s"
      },
      "source": [
        "# 카카오맵 크롤링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uZnhhUwpp6w"
      },
      "outputs": [],
      "source": [
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# 크롬 옵션\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('headless')\n",
        "options.add_argument('--disable-gpu--')\n",
        "options.add_argument('lang=ko_KR')\n",
        "\n",
        "# 크롬 드라이버 설치\n",
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "\n",
        "# 카카오맵 방문\n",
        "driver.get(\"https://map.kakao.com/\")\n",
        "\n",
        "# 페이지 로드시간 암묵적 대기\n",
        "driver.implicitly_wait(10)\n",
        "\n",
        "# csv 입력받을 DataFrame\n",
        "dog_map_df = pd.DataFrame(columns=('name','star','adr','category'))\n",
        "\n",
        "# 검색 할 목록\n",
        "search_list = ['서울 애견 미용실','서울 애견호텔', '서울 반려동물 미용실', '서울 동물약국',\\\n",
        "                 '서울 동물병원', '서울 강아지 수제간식','서울 애견 수제간식', '서울 애견동반', '서울 강아지유치원','서울 애견유치원', '서울 애견용품']\n",
        "\n",
        "# 목록에서 하나씩 검색\n",
        "for search_id in search_list:\n",
        "    # 검색상자 찾기\n",
        "    search_box = driver.find_element(By.CSS_SELECTOR, \"#search\\.keyword\\.query\")\n",
        "    # 검색할 단어 입력\n",
        "    search_box.send_keys(f\"{search_id}\")\n",
        "    time.sleep(1)\n",
        "    # 검색버튼 누르기\n",
        "    search_box.send_keys(Keys.ENTER)\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 지도설정 팝업 제거(최초 1회만 나타나서 예외구문 적용)\n",
        "    try:\n",
        "        driver.find_element(By.CSS_SELECTOR, \"body > div.coach_layer.coach_layer_type1 > div > div > div > span\").click()\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 최초에 페이지번호 뜨지 않고 더보기 눌러줘야함(최초 1회만 나타나서 예외구문 적용)\n",
        "    try:\n",
        "        more_btn = driver.find_element(By.CSS_SELECTOR, \"#info\\.search\\.place\\.more\")\n",
        "        more_btn.click()\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 검색결과의 총 개수 / 75(5페이지 당 요소 개수)\n",
        "    all_page = \"document.querySelector('.keywordSearch').innerHTML\"\n",
        "    page_raw = driver.execute_script(\"return \" + all_page)\n",
        "    page_html = BeautifulSoup(page_raw, 'html.parser')\n",
        "    all_page_html = page_html.find('em', class_=\"cnt\", id=\"info.search.place.cnt\").text\n",
        "    all_page_html = re.sub(r'[^0-9]', '', all_page_html)\n",
        "    all_page_num = math.ceil(int(all_page_html.text) / 75)\n",
        "\n",
        "    # dog_map_df에 입력받을 컬럼\n",
        "    name = []\n",
        "    star = []\n",
        "    adr = []\n",
        "    category = []         \n",
        "    \n",
        "    # 크롤링 시작\n",
        "    # 페이지당 15개의 개체가 있다.\n",
        "    # 이전, 페이지 1~5, 다음 버튼이 존재하며, 이전,다음버튼은 5개씩 넘어가게 되어있음\n",
        "    \n",
        "    # 다음버튼 누르는 횟수 개체 개수 = (15개 x 5page x 다음버튼 횟수)\n",
        "    for p in range(all_page_num):\n",
        "        # 1page ~ 5page 페이지이동 (6~10, 11~15 ...)\n",
        "        # page버튼이 모자르면 종료(6~10구간에서 6,7,8까지만 있으면 8page에서 종료)\n",
        "        for j in range(1,6):\n",
        "            try:\n",
        "                next_btn_1stp = driver.find_element(By.CSS_SELECTOR, f\"#info\\.search\\.page\\.no{j}\")\n",
        "                next_btn_1stp.click()\n",
        "            except:\n",
        "                print(f'{search_id}수집 끝')\n",
        "                break\n",
        "            time.sleep(1)\n",
        "\n",
        "            # 추출할 범위를 CSS로 선택해서 HTML형식으로 변환\n",
        "            js_script = \"document.querySelector('.placelist').innerHTML\"\n",
        "            # 자바스크립트형식 읽어들이기\n",
        "            raw = driver.execute_script(\"return \" + js_script)\n",
        "            html = BeautifulSoup(raw, 'html.parser')\n",
        "            time.sleep(2)\n",
        "\n",
        "            # 직접적인 데이터가 있는 CSS그룹 선택\n",
        "            contents = html.find_all(class_='PlaceItem clickArea')\n",
        "            time.sleep(2)\n",
        "\n",
        "            # CSS그룹에서 필요한 값 추출\n",
        "            for i in contents:\n",
        "                cr_name = i.find(class_='link_name').text\n",
        "                name.append(cr_name)\n",
        "\n",
        "                try:\n",
        "                    cr_star = float(i.select_one(\"em.num\").text)\n",
        "                    star.append(cr_star)\n",
        "                except:\n",
        "                    star.append(0)\n",
        "\n",
        "                cr_adr = i.select_one('.addr').text.replace('\\n', '').split(' ')\n",
        "                if cr_adr[0] == '서울':\n",
        "                    adr.append(cr_adr[1])\n",
        "\n",
        "                cr_category = i.find(class_='subcategory clickable').text\n",
        "                category.append(cr_category)\n",
        "                \n",
        "\n",
        "        next_btn = driver.find_element(By.XPATH,'//*[@id=\"info.search.page.next\"]')\n",
        "        next_btn.send_keys(Keys.ENTER)\n",
        "\n",
        "   \n",
        "    # 추출한 데이터 입력\n",
        "    search_data = pd.DataFrame({'name':name,'star':star,'adr':adr,'category':category})\n",
        "    dog_map_df = pd.concat([dog_map_df,search_data]) \n",
        "  \n",
        "    # 검색어 지우기\n",
        "    driver.implicitly_wait(3)\n",
        "    search_box.clear()\n",
        "    \n",
        "\n",
        "# 중복제거\n",
        "dog_map_df.drop_duplicates(['name'], keep='first', inplace=True)\n",
        "``\n",
        "# csv파일로 저장\n",
        "dog_map_df.to_csv(path_or_buf=f'./dog_map_data.csv',encoding='utf-8-sig', index=False)\n",
        "\n",
        "# 크롬 웹페이지를 닫음\n",
        "driver.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CTH7Iz2pp6z"
      },
      "source": [
        "### 네이버 API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01MQ6UvQpp6z",
        "outputId": "1ef365d6-9ef3-4f7e-e5e7-db545f1d7636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "서울 동물병원 완료\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from urllib.parse import quote     # 한글 -> ASCII코드\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# 네이버 API 아이디, 비번\n",
        "client_id = \"\"\n",
        "client_secret = \"\"\n",
        "\n",
        "headers = {\n",
        "   'X-Naver-Client-Id':  client_id,\n",
        "   'X-Naver-Client-Secret': client_secret\n",
        "}\n",
        "# API주소\n",
        "url = \"https://openapi.naver.com/v1/search/local.json\"\n",
        "\n",
        "# 검색어 목록\n",
        "search =['서울 동물병원']\n",
        "\n",
        "# 검색어 마다 실행\n",
        "for query in search:\n",
        "    disp = '5'       # 출력개수(없어도됨)\n",
        "    start = '1'      # 시작번호(없어도됨)\n",
        "\n",
        "    params = {'query' : query,\n",
        "            'display' : disp,\n",
        "            'start' : start}\n",
        "\n",
        "    resp = requests.get(url, headers=headers, params=params)\n",
        "\n",
        "    # if resp.status_code ==200:\n",
        "    #   print(resp.json().get('items'))\n",
        "\n",
        "\n",
        "    # 요청결과 저장\n",
        "    search_list = []\n",
        "\n",
        "    for i in range(5):\n",
        "        temp = []\n",
        "        temp.append(resp.json().get('items')[i]['title'])\n",
        "        temp.append(resp.json().get('items')[i]['category'])\n",
        "        temp.append(resp.json().get('items')[i]['address'].split()[0])\n",
        "        temp.append(resp.json().get('items')[i]['address'].split()[1])\n",
        "        temp.append(resp.json().get('items')[i]['mapx'])\n",
        "        temp.append(resp.json().get('items')[i]['mapy'])\n",
        "        temp.append(resp.json().get('items')[i]['link'])\n",
        "        search_list.append(temp)\n",
        "\n",
        "    # csv 저장\n",
        "    # 파일명 /'w'쓰기모드 / 한글인코딩 / 자동줄바꿈 하지않음\n",
        "    f = open(f'{query}.csv', 'w', encoding='utf-8-sig', newline='') \n",
        "    csvwriter = csv.writer(f)\n",
        "\n",
        "     # 컬럼명 입력\n",
        "    csvwriter.writerow(['이름','업종','시','행정구','mapx','mapy','링크주소'])\n",
        "\n",
        "    # 행입력(위에 search_list 값 입력)\n",
        "    for j in search_list: \n",
        "        csvwriter.writerow(j)\n",
        "        \n",
        "    f.close()\n",
        "\n",
        "    # 하나의 검색어 저장이 완료되면 출력\n",
        "    print(f'{query} 완료')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqaVopo2pp61"
      },
      "source": [
        "### 탑텐몰 제품, 가격, 할인률"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciyoLSxcpp62"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "driver = webdriver.Chrome(\"C:/Users/eg287/chromedriver\") # 나의 웹드라이버의 위치(설정-도움말-버전에서 확인 후 최신 드라이버 사용)\n",
        "driver.get(\"https://www.topten10mall.com/kr/front/search/categorySearch.do?ctgNo=37341\") # 웹사이트 방문\n",
        "\n",
        "# 팝업 창 제거\n",
        "# driver.find_element(By.CSS_SELECTOR, \"button#intro_popup_close\").click()\n",
        "driver.implicitly_wait(10) # 페이지 로드시간에 10초 암묵적 대기\n",
        "\n",
        "# 검색창에 검색어 입력하기\n",
        "search_box = driver.find_element(By.CSS_SELECTOR, \"#searchWord\")\n",
        "search_box.send_keys(\"맨투맨\")\n",
        "\n",
        "time.sleep(3)\n",
        "\n",
        "# 검색버튼 누르기\n",
        "search_box.send_keys(Keys.ENTER)\n",
        "# raw = driver.page_source # 현재 랜더링된 페이지의 정보를 가져옴\n",
        "\n",
        "# 추출한 데이터 저장할 DataFrame 생성\n",
        "empty = pd.DataFrame(columns=['name','price','discount'])\n",
        "num = 0 # 추출한 값을 넣을 인덱스번호\n",
        "\n",
        "# 크롤링\n",
        "for p in range(28):\n",
        "    # 5초 delay\n",
        "    time.sleep(2)\n",
        "    \n",
        "    # 자바로 HTML가져오기(추출할 범위)\n",
        "    js_script = \"document.querySelector('#divList').innerHTML\"\n",
        "    raw = driver.execute_script(\"return \" + js_script)\n",
        "    \n",
        "    # requests로 HTML 가져오기\n",
        "    # raw = requests.get(\"https://www.topten10mall.com/kr/front/search/totalSearch.do?searchTerm=%EC%85%94%EC%B8%A0\")\n",
        "\n",
        "    # 파서로 추출할 대상 선택\n",
        "    html = BeautifulSoup(raw, 'html.parser')\n",
        "    contents = html.select('.card-goods__body')\n",
        "\n",
        "    # 추출대상별 구체적으로 추출할 클래스 값 추출\n",
        "    for i in contents:\n",
        "        name = i.select_one('.card-goods__text').text\n",
        "        try:\n",
        "            price = i.select_one('.card-goods__price').text\n",
        "        except:\n",
        "            price = 'NULL'\n",
        "        try:\n",
        "            discount = i.select_one('.card-goods__discount').text\n",
        "        except:\n",
        "            discount = '0%'\n",
        "\n",
        "        # empty 데이터프레임에 추출한 값 넣기\n",
        "        empty_dict = {'name':name, 'price':price, 'discount':discount}\n",
        "        for key,value in empty_dict.items():\n",
        "            empty.loc[num,key] = value\n",
        "        num += 1 # 인덱스 번호 1씩 증가하도록\n",
        "        # print(name, price, discount)\n",
        "        \n",
        "        \n",
        "    # 다음 페이지로 이동(다음페이지 없을때까지)\n",
        "    try:\n",
        "        next_btn = driver.find_element(By.CSS_SELECTOR, '#searchGoods > nav > ul > li:nth-last-child(1)')\n",
        "        next_btn.click()\n",
        "    except:\n",
        "        print(\"데이터 수집 완료\")\n",
        "        break\n",
        "\n",
        "\n",
        "# empty.to_csv(path_or_buf='./크롤링.csv',index=False, encoding = \"utf-8-sig\")\n",
        "len(empty)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 네이버 강아지도 크롤링"
      ],
      "metadata": {
        "id": "mBlN4z4rMCuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "서울지역 url 가져오기"
      ],
      "metadata": {
        "id": "i6nS-Lj5Y8t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "BASE_URL = \"https://campaign.naver.com/save-the-stars/\"\n",
        "\n",
        "def get_page(page_url):\n",
        "    page = requests.get(page_url) # page에 response와 html객체를 담는다\n",
        "    soup = BeautifulSoup(page.content, 'html.parser') # html객체를 .content로 문자열화하고, html.parser로 파싱 (DOM객체)\n",
        "    return soup, page \n",
        "\n",
        "soup, page = get_page(BASE_URL)\n",
        "dogMapList = soup.select(\"#new li.items > a\") \n",
        "# >는 직계자손관계, 단순 띄어쓰기는 자식 손주 증손주 등등 다 가능\n",
        "# select는 element들을 가져와서 list화\n",
        "\n",
        "def get_link(item): \n",
        "    return item.attrs[\"href\"] # attrs(속성)을 가져와라\n",
        "\n",
        "def is_Seoul(item):\n",
        "    return '서울' in item.select(\".sub_tit\")[0].text\n",
        "\n",
        "links = list(map(get_link, list(filter(is_Seoul, dogMapList))))"
      ],
      "metadata": {
        "id": "QsMfIwc2MB6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "가게이름 가져오기"
      ],
      "metadata": {
        "id": "vaj8aei7Y_eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "# 웹드라이버\n",
        "wd = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
        "url = 'https://naver.me/5WBTxFeW'\n",
        "\n",
        "# 브라우저가 url을 방문, 페이지 로드 시간 대기\n",
        "wd.get(url)\n",
        "time.sleep(5)\n",
        "\n",
        "# ======================= [START] html 코드를 가져오기 전 웹을 조작하는 코드 =======================\n",
        "\n",
        "# iframe의 id를 이용해서 frame focus 전환 \n",
        "wd.switch_to.frame(\"myPlaceBookmarkListIframe\")\n",
        "\n",
        "# 스크롤 코드 출처 : https://hello-bryan.tistory.com/194\n",
        "# 스크롤 -> 필요한 만큼 반복문으로 구성\n",
        "\n",
        "# 스크롤 높이 가져옴\n",
        "last_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
        "while True:\n",
        "    # 아래코드는 자바스크립트의 scroll 함수를 사용하기 위해 자바스크립트 코드를 web driver에게 전달\n",
        "    # 자바스크립트 코드의 기능 -> 끝까지 스크롤 다운\n",
        "    wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    time.sleep(2)\n",
        "    \n",
        "    # 스크롤 다운 후 스크롤 높이 다시 가져옴\n",
        "    new_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
        "    \n",
        "    # 이전 height와 새로운 height와 같으면 탈출\n",
        "    # 스크롤 할때마다 전체 height가 커지기 때문에 이전 height와 같다는 뜻은 더 이상 스크롤이 없다는 뜻\n",
        "    if new_height == last_height:\n",
        "        break\n",
        "    last_height = new_height\n",
        "\n",
        "# ======================= [END] html 코드를 가져오기 전 웹을 조작하는 코드 =======================\n",
        "\n",
        "# web driver -> js 코드 전달\n",
        "# web 드라이버한테 내가 필요한 부분의 html 코드를 문자열로 받는게 목표!\n",
        "# 그걸 위해서 내가 필요한 부분을 선택하는 자바스크립트 코드를 web driver에게 전달하는 것\n",
        "js_script = \"document.querySelector(\\\"#__next > div:nth-child(1) > div > div._2qKjD2._2Rfog3 > div > div._3ObMW8\\\").innerHTML\"\n",
        "raw = wd.execute_script(\"return \" + js_script)\n",
        "# print(type(raw))\n",
        "\n",
        "# 내가 원하는 ul 부분의 dom 객체가 html 변수 안에 저장된다.\n",
        "html = BeautifulSoup(raw, \"html.parser\")\n",
        "\n",
        "# li.Q9_u2C 안에 원하는 정보가 있으므로 적절한 css Selector 문법을 활용해 필요한 정보를 얻는다.\n",
        "# title : _2gfp-T\n",
        "# addr : _2EFNlx\n",
        "buttons = html.select(\"li.Q9_u2C span._2gfp-T\")\n",
        "\n",
        "def get_text(item):\n",
        "    return item.text\n",
        "\n",
        "title = list(map(get_text, buttons))\n",
        "\n",
        "# html = wd.find_element(By.CSS_SELECTOR, \"html\")\n",
        "# tag = html.find_element(By.CSS_SELECTOR, \"li.Q9_u2C\")\n",
        "print(len(title))\n",
        "print(title)"
      ],
      "metadata": {
        "id": "j1qxePXrLzrA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}